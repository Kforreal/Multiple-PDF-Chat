# ğŸ“š Chat with Multiple PDFs (RAG App)

A **Retrieval-Augmented Generation (RAG)** web application that allows users to upload **multiple PDF documents** and chat with them using a local LLM powered by **Ollama** and **LangChain**.

Built with **Streamlit**, this app extracts text from PDFs, embeds them into a vector database, and retrieves relevant context to answer user questions accurately.

---

## âœ¨ Features

* ğŸ“„ Upload **multiple PDFs** at once
* ğŸ” Semantic search using **vector embeddings** (FAISS)
* ğŸ’¬ Conversational chat interface with **user & bot bubbles**
* ğŸ§  Local LLM inference using **Ollama (Llama 3)**
* âš¡ Fast responses with chunking & retrieval optimization
* ğŸ“Š Sidebar status updates (e.g. *Processing complete*)

---

## ğŸ—ï¸ Tech Stack

* **Python 3.10+**
* **Streamlit** â€“ Web UI
* **LangChain** â€“ RAG pipeline & memory
* **Ollama** â€“ Local LLM (Llama 3)
* **FAISS** â€“ Vector similarity search
* **HuggingFace Embeddings** â€“ Text embeddings
* **PyPDF2** â€“ PDF text extraction

---

## ğŸ“‚ Project Structure

```
multiple-pdf-chat/
â”‚â”€â”€ app.py                 # Main Streamlit app
â”‚â”€â”€ requirements.txt       # Python dependencies
â”‚â”€â”€ README.md              # Project documentation
â”‚â”€â”€ .gitignore             # Ignored files (venv, .env, etc.)
â”‚â”€â”€ .env                   # API keys (NOT committed)
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/multiple-pdf-chat.git
cd multiple-pdf-chat
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate   # Windows
# source venv/bin/activate  # macOS / Linux
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ¤– Ollama Setup (Required)

1. Install Ollama: [https://ollama.com](https://ollama.com)
2. Pull the Llama 3 model:

```bash
ollama pull llama3
```

3. Make sure Ollama is running:

```bash
ollama run llama3
```

---

## ğŸ” Environment Variables

Create a `.env` file in the project root:

```env
HUGGINGFACEHUB_API_TOKEN=your_huggingface_token
```

> âš ï¸ **Never commit `.env` files to GitHub**

---

## â–¶ï¸ Run the App

```bash
streamlit run app.py
```

Then open:

```
http://localhost:8501
```

---

## ğŸ§  How It Works (RAG Flow)

1. User uploads PDFs
2. Text is extracted and chunked
3. Chunks are embedded and stored in FAISS
4. User asks a question
5. Relevant chunks are retrieved
6. LLM generates an answer using retrieved context

---

## ğŸš€ Future Improvements

* ğŸ”„ Persistent vector store (disk-based FAISS)
* ğŸ—‚ï¸ PDF source citations in answers
* ğŸŒ Cloud deployment (Streamlit Cloud)
* ğŸ§© Support for more file formats
* ğŸ” Advanced retriever tuning

---

## ğŸ§‘â€ğŸ’» Author

Built by **YiQi Xiang**
ğŸ“ University of Waterloo â€“ Statistics / Computer Science
ğŸ’¼ Interests: Data, AI, ML, Backend Systems

---

## â­ Acknowledgements

* LangChain Documentation
* Ollama Community
* HuggingFace Transformers

---

If you find this project useful, feel free to â­ the repo!
